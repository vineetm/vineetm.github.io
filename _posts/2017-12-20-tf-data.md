---
layout: post
title:
tags:
---
In this post, we will look at how to **convert text data into a tensor**. [`tf.data`](https://www.tensorflow.org/programmers_guide/datasets) is the [recommended method](https://www.tensorflow.org/api_guides/python/threading_and_queues) to feed data to your tensorflow model and is [now core part](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md) of [tensorflow](https://www.tensorflow.org).

Some tensorflow developers might find this strange. Why do we need this `tf.data`? Isn't creating [placeholders](https://www.tensorflow.org/api_docs/python/tf/placeholder) and `feed_dict` the way to do it? Well, you can certainly use feed_dict, but that requires you to completely pre-process your data, write a separate batching function. Also,`feed_dict` [does not scale well](https://www.tensorflow.org/performance/performance_guide).
>While feeding data using a feed_dict offers a high level of flexibility, in most instances using feed_dict does not scale optimally. However, in instances where only a single GPU is being used the difference can be negligible. Using the Dataset API is still strongly recommended.

Surprisingly, I found [sparse and unclear documentation](https://www.tensorflow.org/versions/master/api_docs/python/tf/data/TextLineDataset) on how to use dataset API for text! I picked up how to use this powerful API from
[NMT](https://github.com/tensorflow/nmt) blog and [code](https://github.com/tensorflow/nmt/blob/master/nmt/utils/iterator_utils.py), and experimenting using the good old jupyter notebook. I have found this method of experimentation using notebook extremely useful for writing code in tensorflow. Thus, a more detailed version of this post is available as a notebook. Feel free to download the notebook, and add your own code to thoroughly understand how these APIs work.

OK, enough motivation. Let us get started!

#### Download and explore data
Well, all text experiments begin with data. So let us setup data. We will work with a small file `sample.en` with 100 sentences in this post. Feel free to change `sample.en` to `train.en` and experiment.

* Download the english sentences `train.en` and vocabulary file `vocab.en` from [Ted Talks dataset IWSLT'15 English-Vietnamese data](https://nlp.stanford.edu/projects/nmt/). Further extract top 100 sentences to `sample.en`
  ```
  wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en
  wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en
  head -100 train.en > sample.en
  ```

* Check the number of lines and make sure you see same output as indicated below
  ```
  wc -l *.en

  100 sample.en
  133317 train.en
  17191 vocab.en
  150608 total
  ```
* Take a look at the first two lines of `sample.en`. You would notice that each line contains a single sentence. Further, each sentence seems tokenized.

  ```
  head -2 sample.en

  Rachel Pike : The science behind a climate headline
  In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .
  ```

* Now, take a look at first 10 lines of `vocab.en`. You would notice that each line contains one word.
  ```
  head vocab.en

  <unk>
  <s>
  </s>
  Rachel
  :
  The
  science
  behind
  a
  climate
  ```

  #### Step1: Create a data iterator
